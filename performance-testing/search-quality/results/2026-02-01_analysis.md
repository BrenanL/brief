# Search Quality Benchmark — Analysis & Findings

**Date:** 2026-02-01
**Run:** `results/2026-02-01_145607.json`
**Corpus:** Brief's own codebase (~55 Python files, 70 LLM descriptions)

---

## Headline Results

| Metric | Keyword Only | Lite Embeddings | Full (LLM) Embeddings |
|--------|-------------|----------------|----------------------|
| **MRR** | 0.628 | **0.747** | **0.749** |
| Recall@5 | 0.740 | 0.760 | 0.840 |
| Hit Rate | 0.800 | 0.800 | 0.880 |

**By tier (MRR):**

| Tier | Keyword | Lite | Full |
|------|---------|------|------|
| Code Terms (10 queries) | 0.850 | **0.925** | **0.925** |
| Natural Language (10 queries) | 0.617 | **0.838** | 0.770 |
| Abstract (5 queries) | 0.207 | 0.209 | **0.354** |

---

## Per-Query Breakdown

Out of 25 queries:
- **18 queries (72%)**: Identical MRR across all three configs
- **Lite beats keyword on 9 queries**, keyword beats lite on 2
- **Full beats keyword on 10 queries**, keyword beats full on 0
- **Full beats lite on 3 queries**, lite beats full on 4, tied on 18

### Where Full LLM Descriptions Beat Lite

All 3 are abstract-tier queries:

| Query | Lite MRR | Full MRR | Delta |
|-------|----------|----------|-------|
| "making code follow project conventions automatically" | 0.143 | 0.500 | +0.357 |
| "giving AI agents the right information to write correct code" | 0.111 | 0.500 | +0.389 |
| "understanding how a feature is implemented end to end" | 0.125 | 0.143 | +0.018 |

### Where Lite Beats Full

| Query | Lite MRR | Full MRR | Delta |
|-------|----------|----------|-------|
| "tracking what the agent is currently working on" | 1.000 | 0.500 | -0.500 |
| "analyzing code structure without running it" | 0.125 | 0.000 | -0.125 |
| "storing and recalling project-specific knowledge" | 0.250 | 0.200 | -0.050 |
| "preventing the agent from losing its place..." | 0.167 | 0.125 | -0.042 |

### Queries All Configs Miss

| Query | Tier | Expected | Best Result |
|-------|------|----------|-------------|
| "analyzing code structure without running it" | natural_language | parser.py, manifest.py | generator.py (wrong) |
| "preventing the agent from losing its place..." | abstract | manager.py | context.py (wrong) |
| "understanding how a feature is implemented end to end" | abstract | tracer.py | relationships.py (wrong) |

---

## What These Results Mean

### Finding 1: Lite descriptions are sufficient for file discovery

For the purpose of search ranking (finding the right files), lite AST-derived descriptions perform nearly identically to full LLM descriptions. Overall MRR delta is +0.002. This validates using lite descriptions as the default in `brief setup`.

### Finding 2: Embeddings meaningfully improve over keyword-only

Both embedding configs improve MRR by ~+0.119 over keyword-only. 9 queries improved, 2 regressed, 14 were already correct. The improvement is real and consistent across code-term and natural-language tiers.

### Finding 3: LLM descriptions help on abstract queries

The only tier where full LLM descriptions meaningfully outperform lite is abstract queries (0.354 vs 0.209 MRR). These are conceptual queries that require understanding what code *does*, not just what it's *named*. But even full descriptions only reach 0.354 MRR — this is a search architecture limitation, not just a description quality issue.

---

## Limitations of This Benchmark

### 1. Small corpus

55 files means keyword search is already effective — there aren't enough files to create ambiguity. On a 500-file codebase, description quality would matter more because there are more files to disambiguate between. This benchmark likely understates the value of both embeddings and LLM descriptions.

### 2. Hybrid search dilutes the description effect

Hybrid search uses 70% semantic + 30% keyword weighting. The keyword component provides a strong baseline signal that makes it harder to see the marginal impact of better embeddings. On 18/25 queries, the keyword component alone finds the right file — the semantic component doesn't need to help.

### 3. Query bias

The benchmark queries were written by someone who knows the codebase, which likely biases toward terms that overlap with code vocabulary. Real users would use more diverse language, especially for natural language and abstract queries. The sample size (25 queries, only 5 abstract) is also small.

### 4. Single expected files

Most queries have only 1 expected file, making scoring binary. Doesn't capture scenarios where the search returns something useful but not the exact expected file.

### 5. Only measures search ranking, not context quality

**This is the most important limitation.** LLM descriptions serve two purposes:
1. **Search ranking** (finding files) — measured here
2. **Context output** (explaining files to the agent) — NOT measured

When lite and full find the same files, the agent receives very different context packages:
- **Lite**: "class TaskManager, methods: create_task, get_task... Imports from: storage.py, config.py"
- **Full LLM**: "Manages task lifecycle with JSONL persistence, enabling agents to resume work after context compaction. The active_task mechanism tracks what the agent is working on, and task notes provide a way to persist observations before context window limits are reached."

The semantic richness in the output — understanding *why* code exists, *how* pieces relate, *what problems* they solve — is arguably more valuable than the search ranking improvement. Testing whether this leads to better agent behavior requires the agent-based performance testing framework, not this search benchmark.

---

## Conclusions

### What we CAN conclude

- Lite descriptions are viable for search/file discovery during setup
- Embeddings (lite or full) meaningfully improve search over keyword-only
- Abstract/conceptual queries are a known weak point needing architectural work
- LLM descriptions provide marginal search improvement (+0.002 MRR) on this corpus

### What we CANNOT conclude

- "LLM descriptions don't do anything" — this test doesn't measure their output quality value
- "Lite is as good as full" — only for search ranking on a small corpus
- "We should stop generating LLM descriptions" — they may still improve agent performance through richer context, even if search ranking is similar

---

## Recommendations

### For the first-user experience (immediate)

1. **Use lite descriptions + embeddings as the default setup path.** Search quality is nearly identical to full LLM descriptions, with zero LLM cost and seconds instead of minutes.
2. **Present LLM descriptions as an upgrade.** "Run `brief describe batch` for richer context" — position it as an enhancement, not a prerequisite.
3. **This gives users semantic search immediately** without API keys for descriptions (only OpenAI for embeddings).

### For the benchmark tool (short-term)

1. **Test on a larger codebase** — clone an open-source project (200-500 files), run `brief analyze all`, generate both description types, re-run the benchmark. This will show whether the lite/full gap widens with corpus size.
2. **Add a pure semantic search config** — test with `search_fn: "semantic"` to isolate description quality from keyword blending.
3. **Add more abstract/conceptual queries** — expand to 15-20 abstract queries to get a more reliable signal on that tier.
4. **Test with varying hybrid weights** — add configs with 50/50, 80/20, and 90/10 semantic/keyword splits to understand the keyword influence.

### For understanding LLM description value (medium-term)

5. **Run agent-based performance tests** comparing lite-only vs full descriptions. Use the existing orchestrator to measure: does richer context output lead to better task completion, fewer errors, less tool usage? This is the test that actually answers "do LLM descriptions matter?"
6. **Add a "context quality" metric** to this benchmark — beyond "did we find the right file," measure whether the returned context package contains enough information to understand the code (could use an LLM judge or manual scoring).

### For search architecture (longer-term)

7. **Investigate chunked embeddings** — per-function/class embeddings instead of per-file, for better granularity on large files.
8. **Explore query expansion** — transform abstract queries into code-term queries before searching (e.g., "preventing context loss" → expand to "task resume active_task save state").
9. **Consider re-ranking** — use a lightweight re-ranker after initial retrieval to improve ranking quality.

---

## Appendix: Test Infrastructure

- **Benchmark tool**: `performance-testing/search-quality/runner.py`
- **Queries**: `performance-testing/search-quality/benchmark.json`
- **Configs**: `performance-testing/search-quality/configs.py`
- **Scoring**: `performance-testing/search-quality/scoring.py`
- **Lite generator**: `src/brief/generation/lite.py`
- **Full results**: `results/2026-02-01_145607.json`
- **Cost of this run**: ~$0.01 (OpenAI embeddings)
